{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the necessary modules and libraries for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these lines in terminal and remember to set the environment variable to this ipynb file\n",
    "# conda create --name comp.sgn.120 python=3.11.3\n",
    "# conda activate comp.sgn.120\n",
    "# conda install numpy=1.26.2\n",
    "# pip install ipykernel --upgrade\n",
    "# conda install -c conda-forge ffmpeg\n",
    "\n",
    "# Uncomment these lines to install the required packages if you haven't already\n",
    "# !pip install pydub==0.25.1\n",
    "# !pip install tqdm==4.66.1\n",
    "# !pip install librosa==0.10.1\n",
    "# !pip install matplotlib==3.7.2\n",
    "# !pip install scikit-learn==1.3.2\n",
    "# !pip install scipy==1.11.4 \n",
    "# !pip install pandas==2.1.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Modules and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Database loading and Feature extraction\n",
    "from pydub import AudioSegment\n",
    "import librosa as lb\n",
    "import librosa.display\n",
    "from scipy.stats import skew\n",
    "from scipy.signal import hamming, hann\n",
    "\n",
    "\n",
    "# Representation\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFolder(folder):\n",
    "    folder_names = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for name in dirs:\n",
    "            folder_names.append(os.path.join(root, name))\n",
    "    return folder_names\n",
    "\n",
    "\n",
    "def readFileInFolder(folder):\n",
    "    file_lists = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for name in files:\n",
    "            file_lists.append(os.path.join(root, name))\n",
    "    return file_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMFCC(name, path):\n",
    "    n_mel=40\n",
    "    hop_size=128\n",
    "    n_fft=512\n",
    "    data, _ = librosa.core.load(name, sr = None)\n",
    "    try:\n",
    "        ft1 = lb.feature.mfcc(y= data, n_mfcc=n_mel, hop_length=hop_size, norm=\"ortho\", n_fft=n_fft)\n",
    "        ft2 = librosa.feature.zero_crossing_rate(y = data)[0]\n",
    "        ft3 = librosa.feature.spectral_rolloff(y= data)[0]\n",
    "        ft4 = librosa.feature.spectral_centroid(y = data)[0]\n",
    "        ft5 = librosa.feature.spectral_contrast(y = data)[0]\n",
    "        ft6 = librosa.feature.spectral_bandwidth(y = data)[0]\n",
    "        ft1_trunc = np.hstack((np.mean(ft1, axis=1), np.std(ft1, axis=1), skew(ft1, axis = 1), np.max(ft1, axis = 1), np.median(ft1, axis = 1), np.min(ft1, axis = 1)))\n",
    "        ft2_trunc = np.hstack((np.mean(ft2), np.std(ft2), skew(ft2), np.max(ft2), np.median(ft2), np.min(ft2)))\n",
    "        ft3_trunc = np.hstack((np.mean(ft3), np.std(ft3), skew(ft3), np.max(ft3), np.median(ft3), np.min(ft3)))\n",
    "        ft4_trunc = np.hstack((np.mean(ft4), np.std(ft4), skew(ft4), np.max(ft4), np.median(ft4), np.min(ft4)))\n",
    "        ft5_trunc = np.hstack((np.mean(ft5), np.std(ft5), skew(ft5), np.max(ft5), np.median(ft5), np.min(ft5)))\n",
    "        ft6_trunc = np.hstack((np.mean(ft6), np.std(ft6), skew(ft6), np.max(ft6), np.median(ft6), np.max(ft6)))\n",
    "        return pd.Series(np.hstack((ft1_trunc, ft2_trunc, ft3_trunc, ft4_trunc, ft5_trunc, ft6_trunc)))\n",
    "    except:\n",
    "        print('bad file')\n",
    "        return pd.Series([0]*210)\n",
    "\n",
    "def extractOtherFeatures(y, sr, path=None):\n",
    "    features = []\n",
    "    \n",
    "    n_mel=40\n",
    "    hop_size=128\n",
    "    n_fft=512\n",
    "    \n",
    "    # Energy \n",
    "    energy = np.sum(np.power(y, 2))\n",
    "    # RMS\n",
    "    rms = np.sqrt(np.sum(np.power(y, 2)))\n",
    "    # Spectrograms\n",
    "    spec = np.abs(lb.stft(y, n_fft=n_fft, hop_length=hop_size))\n",
    "    # Mel Spectrogram\n",
    "    mel = lb.feature.melspectrogram(S=spec, n_mels=n_mel)\n",
    "    # Log Mel Spectrogram\n",
    "    logmel = lb.power_to_db(mel)\n",
    "    # CQT Spectrogram\n",
    "    cqt = np.abs(lb.cqt(y, sr=sr, hop_length=hop_size, n_bins=40, bins_per_octave=12))   \n",
    "\n",
    "    features.append(energy)\n",
    "    features.append(rms)\n",
    "    features.append(spec)\n",
    "    features.append(mel)\n",
    "    features.append(logmel)\n",
    "    features.append(cqt)    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAccuracy(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall = recall_score(y_test, y_pred, average = 'macro')\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tram_Train: https://freesound.org/people/publictransport/packs/36726/\n",
    "# Tram_Train: https://freesound.org/people/ali.abdelsalam/packs/36722/\n",
    "# Bus_Train: https://freesound.org/people/emmakyllikki/packs/36810/\n",
    "# Bus_Train: https://freesound.org/people/glingden/packs/36807/\n",
    "# Tram_Test: My own recording\n",
    "# Bus_Test: My own recording\n",
    "folder_list = readFolder(\"audio\")\n",
    "folder_to_read = [\"Bus_Test\", \"Bus_Train\", \"Tram_Test\", \"Tram_Train\"]\n",
    "bus_test = []\n",
    "bus_train = []\n",
    "tram_test = []\n",
    "tram_train = []\n",
    "label = {}\n",
    "for folder in folder_list:\n",
    "    # Read all the files and append to the list of files\n",
    "    files = readFileInFolder(folder)\n",
    "    for name in folder_to_read:\n",
    "        if name in folder:\n",
    "            # Append the files to the corresponding list\n",
    "            if name == \"Bus_Test\":\n",
    "                bus_test = files\n",
    "                for file in files:\n",
    "                    label[file] = \"bus\"\n",
    "            elif name == \"Bus_Train\":\n",
    "                bus_train = files\n",
    "                for file in files:\n",
    "                    label[file] = \"bus\"\n",
    "            elif name == \"Tram_Test\":\n",
    "                tram_test = files\n",
    "                for file in files:\n",
    "                    label[file] = \"tram\"\n",
    "            elif name == \"Tram_Train\":\n",
    "                tram_train = files\n",
    "                for file in files:\n",
    "                    label[file] = \"tram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:28<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading train mfcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_10424\\500016299.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, _ = librosa.core.load(name, sr = None)\n",
      "c:\\Users\\nguye\\.conda\\envs\\comp.sgn.120\\Lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "100%|██████████| 18/18 [00:09<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading test mfcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "train_data = pd.DataFrame()\n",
    "train_data[\"fname\"] = bus_train + tram_train\n",
    "test_data = pd.DataFrame()\n",
    "test_data[\"fname\"] = bus_test + tram_test\n",
    "\n",
    "train_data = train_data[\"fname\"].progress_apply(getMFCC, path = None)\n",
    "print(\"done loading train mfcc\")\n",
    "test_data = test_data[\"fname\"].progress_apply(getMFCC, path = None)\n",
    "print(\"done loading test mfcc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n"
     ]
    }
   ],
   "source": [
    "train_data[\"fname\"] = bus_train + tram_train\n",
    "train_data[\"label\"] = train_data[\"fname\"].apply(lambda x: label[x])\n",
    "\n",
    "print(\"Train data:\")\n",
    "# print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n"
     ]
    }
   ],
   "source": [
    "test_data[\"fname\"] = bus_test + tram_test\n",
    "test_data[\"label\"] = test_data[\"fname\"].apply(lambda x: label[x])\n",
    "\n",
    "print(\"Test data:\")\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Random Foresth using MFCC ttps://www.kaggle.com/amlanpraharaj/random-forest-using-mfcc-features\n",
    "X = train_data.drop(['label', 'fname'], axis=1)\n",
    "feature_names = list(X.columns)\n",
    "X = X.values\n",
    "\n",
    "labels = np.sort(np.unique(train_data.label.values))\n",
    "\n",
    "num_class = len(labels)\n",
    "c2i = {}\n",
    "i2c = {}\n",
    "for i, c in enumerate(labels):\n",
    "    c2i[c] = i\n",
    "    i2c[i] = c\n",
    "y = np.array([c2i[x] for x in train_data.label.values])\n",
    "X_test = test_data.drop(['label', 'fname'], axis=1).values\n",
    "y_test = np.array([c2i[x] for x in test_data.label.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that exporting the csv file from the beginning of the\n",
    "def exportCSV(y_pred, y_test, filename, bus_test = bus_test, tram_test = tram_test, i2c = i2c):\n",
    "    # convert the binary data into a class label (bus or tram)\n",
    "    y_pred_label = []\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred_label.append(i2c[y_pred[i]])\n",
    "    y_pred_label = np.array(y_pred_label)\n",
    "    # print(y_pred_label)\n",
    "\n",
    "    # convert the binary data into a class label (bus or tram)\n",
    "    y_test_label = []\n",
    "    for i in range(len(y_test)):\n",
    "        y_test_label.append(i2c[y_test[i]])\n",
    "    y_test_label = np.array(y_test_label)\n",
    "    # print(y_test_label)\n",
    "    y_test_name = []\n",
    "    for i in range(len(bus_test)):\n",
    "        y_test_name.append(os.path.basename(bus_test[i]))\n",
    "    for i in range(len(tram_test)):\n",
    "        y_test_name.append(os.path.basename(tram_test[i]))\n",
    "    y_test_name = np.array(y_test_name)\n",
    "    # print(y_test_name)\n",
    "\n",
    "    # Export the CSV file using y_test_name, y_pred_label, and y_test_label\n",
    "    df = pd.DataFrame({'fname': y_test_name, 'y_pred': y_pred_label, 'y_test': y_test_label})\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocessing(X, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    pca = PCA(n_components=65).fit(X_scaled)\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    print(sum(pca.explained_variance_ratio_))\n",
    "    return X, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9828026391973513\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA for dimension reduction\n",
    "pca = PCA(n_components=65).fit(X_scaled)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(sum(pca.explained_variance_ratio_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcessAndTrainAndEvaluate(X_pca, y, feature = \"MFCC\"):\n",
    "    # Build a KNN model\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size = 0.01, random_state = 42, shuffle = True)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Test the kNN model with the test data\n",
    "    y_pred = knn.predict(X_test_pca)\n",
    "    print(\"KNN: \")\n",
    "    printAccuracy(y_test, y_pred)\n",
    "    exportCSV(y_pred, y_test, 'KNN_output.csv')\n",
    "    \n",
    "    # Build a SVM model\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size = 0.01, random_state = 42, shuffle = True)\n",
    "    clf = SVC(kernel = 'rbf', probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Test the SVM model with the test data\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    print(\"SVM: \")\n",
    "    printAccuracy(y_test, y_pred)\n",
    "    exportCSV(y_pred, y_test, 'SVM_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7222222222222222\n",
      "Precision:  0.8333333333333333\n",
      "Recall:  0.6875\n",
      "Accuracy:  0.7222222222222222\n",
      "Precision:  0.7207792207792207\n",
      "Recall:  0.7125\n"
     ]
    }
   ],
   "source": [
    "dataProcessAndTrainAndEvaluate(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the audio files\n",
    "bus_test_audio = []\n",
    "bus_train_audio = []\n",
    "tram_test_audio = []\n",
    "tram_train_audio = []\n",
    "for file in bus_train:\n",
    "    y, sr = lb.load(file, sr=None)\n",
    "    bus_train_audio.append((y, sr))\n",
    "for file in tram_train:\n",
    "    y, sr = lb.load(file, sr=None)\n",
    "    tram_train_audio.append((y, sr))\n",
    "\n",
    "# Read the audio files in m4a format\n",
    "for file in bus_test:\n",
    "    sound = AudioSegment.from_file(file, format=\"m4a\")\n",
    "    sound.export(\"temp.wav\", format=\"wav\")\n",
    "    y, sr = lb.load(\"temp.wav\", sr=None)\n",
    "    bus_test_audio.append((y, sr))\n",
    "    os.remove(\"temp.wav\")\n",
    "for file in tram_test:\n",
    "    sound = AudioSegment.from_file(file, format=\"m4a\")\n",
    "    sound.export(\"temp.wav\", format=\"wav\")\n",
    "    y, sr = lb.load(\"temp.wav\", sr=None)\n",
    "    tram_test_audio.append((y, sr))\n",
    "    os.remove(\"temp.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of bus train audio files: \", len(bus_train_audio))\n",
    "print(\"Number of tram train audio files: \", len(tram_train_audio))\n",
    "print(\"Number of bus test audio files: \", len(bus_test_audio))\n",
    "print(\"Number of tram test audio files: \", len(tram_test_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tram_features = []\n",
    "bus_features = []\n",
    "\n",
    "tram_test_features = []\n",
    "bus_test_features = []\n",
    "# using the data from tram_train_audio and bus_train_audio\n",
    "for y, sr in tram_train_audio:\n",
    "    features = extractOtherFeatures(y, sr)\n",
    "    tram_features.append(features)\n",
    "\n",
    "for y, sr in bus_train_audio:\n",
    "    features = extractOtherFeatures(y, sr)\n",
    "    bus_features.append(features)\n",
    "\n",
    "# using the data from tram_test_audio and bus_test_audio\n",
    "for y, sr in tram_test_audio:\n",
    "    features = extractOtherFeatures(y, sr)\n",
    "    tram_test_features.append(features)\n",
    "    \n",
    "for y, sr in bus_test_audio:\n",
    "    features = extractOtherFeatures(y, sr)\n",
    "    bus_test_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tram_features)) # 59\n",
    "print(len(tram_features[0])) # 6\n",
    "print(len(bus_features)) # 39\n",
    "print(len(bus_features[0])) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing with other features\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "features = tram_features + bus_features\n",
    "features_test = tram_test_features + bus_test_features\n",
    "feature_names = [\"energy\", \"rms\", \"spec\", \"mel\", \"logmel\", \"cqt\"]\n",
    "for i in range(len(bus_features[0])): # 6\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    train_data[\"fname\"] = bus_train + tram_train\n",
    "    test_data[\"fname\"] = bus_test + tram_test\n",
    "    # for j in range(len(features)): # 98\n",
    "    train_data[\"label\"] = train_data[\"fname\"].apply(lambda x: label[x])\n",
    "    test_data[\"label\"] = test_data[\"fname\"].apply(lambda x: label[x])\n",
    "\n",
    "    # take the 0 element of each feature[i]\n",
    "    train_data[feature_names[i]] = [x[i] for x in features]\n",
    "    test_data[feature_names[i]] = [x[i] for x in features_test]\n",
    "    \n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)      \n",
    "\n",
    "# concatenate the features_names into 1 dataframe, with keeping the fname and label columns and not concatenating them\n",
    "train_data = pd.concat(train_data_list, axis=1)\n",
    "test_data = pd.concat(test_data_list, axis=1)\n",
    "\n",
    "# fix the overlapping columns\n",
    "train_data = train_data.loc[:,~train_data.columns.duplicated()]\n",
    "test_data = test_data.loc[:,~test_data.columns.duplicated()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Train data:\")\n",
    "# print(train_data)\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data:\")\n",
    "# print(test_data)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
